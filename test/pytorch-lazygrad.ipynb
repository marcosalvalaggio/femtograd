{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1aeddf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "81b061d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(414, 6)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x = pd.read_csv(\"/Users/marcosalvalaggio/code/lazygrad/test/data/x.csv\", sep=\";\")\n",
    "df_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "723b292f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1 transaction date</th>\n",
       "      <th>X2 house age</th>\n",
       "      <th>X3 distance to the nearest MRT station</th>\n",
       "      <th>X4 number of convenience stores</th>\n",
       "      <th>X5 latitude</th>\n",
       "      <th>X6 longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.823683</td>\n",
       "      <td>1.255628</td>\n",
       "      <td>-0.792495</td>\n",
       "      <td>2.007407</td>\n",
       "      <td>1.125430</td>\n",
       "      <td>0.448762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.823683</td>\n",
       "      <td>0.157086</td>\n",
       "      <td>-0.616612</td>\n",
       "      <td>1.667503</td>\n",
       "      <td>0.912444</td>\n",
       "      <td>0.401139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.541151</td>\n",
       "      <td>-0.387791</td>\n",
       "      <td>-0.414015</td>\n",
       "      <td>0.307885</td>\n",
       "      <td>1.486860</td>\n",
       "      <td>0.688183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.246435</td>\n",
       "      <td>-0.387791</td>\n",
       "      <td>-0.414015</td>\n",
       "      <td>0.307885</td>\n",
       "      <td>1.486860</td>\n",
       "      <td>0.688183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.121951</td>\n",
       "      <td>-1.117223</td>\n",
       "      <td>-0.549997</td>\n",
       "      <td>0.307885</td>\n",
       "      <td>0.834188</td>\n",
       "      <td>0.592937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X1 transaction date  X2 house age  X3 distance to the nearest MRT station  \\\n",
       "0            -0.823683      1.255628                               -0.792495   \n",
       "1            -0.823683      0.157086                               -0.616612   \n",
       "2             1.541151     -0.387791                               -0.414015   \n",
       "3             1.246435     -0.387791                               -0.414015   \n",
       "4            -1.121951     -1.117223                               -0.549997   \n",
       "\n",
       "   X4 number of convenience stores  X5 latitude  X6 longitude  \n",
       "0                         2.007407     1.125430      0.448762  \n",
       "1                         1.667503     0.912444      0.401139  \n",
       "2                         0.307885     1.486860      0.688183  \n",
       "3                         0.307885     1.486860      0.688183  \n",
       "4                         0.307885     0.834188      0.592937  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ce5ba0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(414, 6)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "xs = df_x.to_numpy()\n",
    "print(xs.shape)\n",
    "print(type(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "847d4b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(414,)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "df_y = pd.read_csv(\"/Users/marcosalvalaggio/code/lazygrad/test/data/y.csv\", sep=\";\")\n",
    "y = df_y.to_numpy().squeeze()\n",
    "print(y.shape)\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "45766cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([414, 6])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([414])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.tensor(xs).float()\n",
    "y_train = torch.tensor(y).float()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# pass the objects into the device selected\n",
    "x_train = x_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "print(x_train.shape)\n",
    "print(type(x_train))\n",
    "print(y_train.shape)\n",
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "667f990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x = x.clone().detach().requires_grad_(True)\n",
    "        self.y = y.clone().detach().requires_grad_(True)\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, ix):\n",
    "        return self.x[ix], self.y[ix]\n",
    "# create a MyDataset object \n",
    "ds = MyDataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "670b5761",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dl = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "# for d in dl: \n",
    "#     print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f8cf96ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_to_hidden_layer = nn.Linear(6,16)\n",
    "        self.hidden_layer_activation = nn.ReLU()\n",
    "        self.hidden_to_output_layer = nn.Linear(16,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_to_hidden_layer(x)\n",
    "        x = self.hidden_layer_activation(x)\n",
    "        x = self.hidden_to_output_layer(x)\n",
    "        return x\n",
    "\n",
    "model = FFNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "506b0a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5556], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test on a single observation before the training phase \n",
    "model(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "33ec233d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.4079,  0.3970,  0.3447,  0.3953, -0.1863, -0.2149],\n",
      "        [ 0.2333,  0.1282, -0.3173,  0.0432, -0.2437, -0.1616],\n",
      "        [-0.0039, -0.3560, -0.2801,  0.2377, -0.3861,  0.0946],\n",
      "        [-0.0898, -0.3485, -0.1534,  0.2909, -0.3358, -0.0444],\n",
      "        [ 0.0915,  0.2905,  0.1560,  0.0029,  0.2739,  0.2538],\n",
      "        [ 0.1270,  0.2603,  0.3003,  0.0648,  0.2491,  0.3452],\n",
      "        [-0.3170, -0.2898,  0.3876,  0.4036,  0.2787, -0.2107],\n",
      "        [ 0.0140,  0.0647, -0.1175,  0.3941, -0.1588,  0.3291],\n",
      "        [ 0.3859, -0.3205,  0.1762, -0.3065,  0.2470,  0.3209],\n",
      "        [-0.3062, -0.2113, -0.3739,  0.1416, -0.4033, -0.3064],\n",
      "        [ 0.0331, -0.3719,  0.3719,  0.2618, -0.0363,  0.1981],\n",
      "        [-0.2202, -0.0986,  0.2350,  0.0638,  0.0998, -0.3080],\n",
      "        [ 0.1519, -0.4040,  0.3979, -0.2688, -0.1959, -0.1863],\n",
      "        [ 0.0133,  0.3794,  0.3701, -0.1442,  0.1990,  0.2392],\n",
      "        [ 0.3979, -0.0029,  0.2865,  0.1847,  0.2432, -0.1437],\n",
      "        [-0.3800,  0.0649,  0.2394, -0.3382, -0.3520,  0.2361]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0417,  0.1683, -0.1886, -0.0895,  0.3332,  0.1039,  0.3712, -0.1712,\n",
      "         0.3474,  0.1228,  0.3273,  0.1885, -0.2371, -0.0957,  0.2831,  0.0686],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1507,  0.2124,  0.1272,  0.1175,  0.0972,  0.0342,  0.2358,  0.1140,\n",
      "         -0.2279, -0.0317,  0.0455, -0.1890,  0.1203,  0.0487, -0.1377,  0.0461]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0097], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "00c7e39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.MSELoss()\n",
    "opt = SGD(model.parameters(), lr = 0.001)\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8fe6d98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "epoch: 1/50 | batch: 1/13 | loss: 1603.51\n",
      "epoch: 1/50 | batch: 2/13 | loss: 1788.71\n",
      "epoch: 1/50 | batch: 3/13 | loss: 1560.08\n",
      "epoch: 1/50 | batch: 4/13 | loss: 1452.57\n",
      "epoch: 1/50 | batch: 5/13 | loss: 1397.21\n",
      "epoch: 1/50 | batch: 6/13 | loss: 1594.43\n",
      "epoch: 1/50 | batch: 7/13 | loss: 1730.82\n",
      "epoch: 1/50 | batch: 8/13 | loss: 1525.70\n",
      "epoch: 1/50 | batch: 9/13 | loss: 1367.21\n",
      "epoch: 1/50 | batch: 10/13 | loss: 1511.87\n",
      "epoch: 1/50 | batch: 11/13 | loss: 1244.56\n",
      "epoch: 1/50 | batch: 12/13 | loss: 1486.57\n",
      "epoch: 1/50 | batch: 13/13 | loss: 1389.65\n",
      "epoch: 2/50 | batch: 1/13 | loss: 1375.18\n",
      "epoch: 2/50 | batch: 2/13 | loss: 1281.45\n",
      "epoch: 2/50 | batch: 3/13 | loss: 1098.01\n",
      "epoch: 2/50 | batch: 4/13 | loss: 1239.14\n",
      "epoch: 2/50 | batch: 5/13 | loss: 1099.27\n",
      "epoch: 2/50 | batch: 6/13 | loss: 1174.15\n",
      "epoch: 2/50 | batch: 7/13 | loss: 931.16\n",
      "epoch: 2/50 | batch: 8/13 | loss: 1388.82\n",
      "epoch: 2/50 | batch: 9/13 | loss: 1088.76\n",
      "epoch: 2/50 | batch: 10/13 | loss: 941.96\n",
      "epoch: 2/50 | batch: 11/13 | loss: 824.52\n",
      "epoch: 2/50 | batch: 12/13 | loss: 944.61\n",
      "epoch: 2/50 | batch: 13/13 | loss: 918.92\n",
      "epoch: 3/50 | batch: 1/13 | loss: 598.60\n",
      "epoch: 3/50 | batch: 2/13 | loss: 552.38\n",
      "epoch: 3/50 | batch: 3/13 | loss: 504.02\n",
      "epoch: 3/50 | batch: 4/13 | loss: 626.13\n",
      "epoch: 3/50 | batch: 5/13 | loss: 738.73\n",
      "epoch: 3/50 | batch: 6/13 | loss: 403.07\n",
      "epoch: 3/50 | batch: 7/13 | loss: 377.21\n",
      "epoch: 3/50 | batch: 8/13 | loss: 262.26\n",
      "epoch: 3/50 | batch: 9/13 | loss: 282.52\n",
      "epoch: 3/50 | batch: 10/13 | loss: 215.06\n",
      "epoch: 3/50 | batch: 11/13 | loss: 226.91\n",
      "epoch: 3/50 | batch: 12/13 | loss: 225.71\n",
      "epoch: 3/50 | batch: 13/13 | loss: 175.53\n",
      "epoch: 4/50 | batch: 1/13 | loss: 410.89\n",
      "epoch: 4/50 | batch: 2/13 | loss: 212.46\n",
      "epoch: 4/50 | batch: 3/13 | loss: 86.51\n",
      "epoch: 4/50 | batch: 4/13 | loss: 149.39\n",
      "epoch: 4/50 | batch: 5/13 | loss: 156.46\n",
      "epoch: 4/50 | batch: 6/13 | loss: 77.72\n",
      "epoch: 4/50 | batch: 7/13 | loss: 54.62\n",
      "epoch: 4/50 | batch: 8/13 | loss: 62.25\n",
      "epoch: 4/50 | batch: 9/13 | loss: 120.75\n",
      "epoch: 4/50 | batch: 10/13 | loss: 75.26\n",
      "epoch: 4/50 | batch: 11/13 | loss: 63.20\n",
      "epoch: 4/50 | batch: 12/13 | loss: 52.31\n",
      "epoch: 4/50 | batch: 13/13 | loss: 67.49\n",
      "epoch: 5/50 | batch: 1/13 | loss: 53.73\n",
      "epoch: 5/50 | batch: 2/13 | loss: 95.57\n",
      "epoch: 5/50 | batch: 3/13 | loss: 85.73\n",
      "epoch: 5/50 | batch: 4/13 | loss: 57.43\n",
      "epoch: 5/50 | batch: 5/13 | loss: 114.91\n",
      "epoch: 5/50 | batch: 6/13 | loss: 73.97\n",
      "epoch: 5/50 | batch: 7/13 | loss: 71.42\n",
      "epoch: 5/50 | batch: 8/13 | loss: 71.34\n",
      "epoch: 5/50 | batch: 9/13 | loss: 54.37\n",
      "epoch: 5/50 | batch: 10/13 | loss: 35.38\n",
      "epoch: 5/50 | batch: 11/13 | loss: 45.24\n",
      "epoch: 5/50 | batch: 12/13 | loss: 273.84\n",
      "epoch: 5/50 | batch: 13/13 | loss: 32.51\n",
      "epoch: 6/50 | batch: 1/13 | loss: 52.61\n",
      "epoch: 6/50 | batch: 2/13 | loss: 98.09\n",
      "epoch: 6/50 | batch: 3/13 | loss: 52.66\n",
      "epoch: 6/50 | batch: 4/13 | loss: 255.36\n",
      "epoch: 6/50 | batch: 5/13 | loss: 31.60\n",
      "epoch: 6/50 | batch: 6/13 | loss: 68.70\n",
      "epoch: 6/50 | batch: 7/13 | loss: 58.00\n",
      "epoch: 6/50 | batch: 8/13 | loss: 62.77\n",
      "epoch: 6/50 | batch: 9/13 | loss: 40.99\n",
      "epoch: 6/50 | batch: 10/13 | loss: 67.32\n",
      "epoch: 6/50 | batch: 11/13 | loss: 50.81\n",
      "epoch: 6/50 | batch: 12/13 | loss: 20.43\n",
      "epoch: 6/50 | batch: 13/13 | loss: 85.42\n",
      "epoch: 7/50 | batch: 1/13 | loss: 48.99\n",
      "epoch: 7/50 | batch: 2/13 | loss: 114.61\n",
      "epoch: 7/50 | batch: 3/13 | loss: 61.78\n",
      "epoch: 7/50 | batch: 4/13 | loss: 55.18\n",
      "epoch: 7/50 | batch: 5/13 | loss: 64.90\n",
      "epoch: 7/50 | batch: 6/13 | loss: 70.96\n",
      "epoch: 7/50 | batch: 7/13 | loss: 36.64\n",
      "epoch: 7/50 | batch: 8/13 | loss: 68.52\n",
      "epoch: 7/50 | batch: 9/13 | loss: 44.25\n",
      "epoch: 7/50 | batch: 10/13 | loss: 230.24\n",
      "epoch: 7/50 | batch: 11/13 | loss: 19.77\n",
      "epoch: 7/50 | batch: 12/13 | loss: 41.14\n",
      "epoch: 7/50 | batch: 13/13 | loss: 50.36\n",
      "epoch: 8/50 | batch: 1/13 | loss: 59.14\n",
      "epoch: 8/50 | batch: 2/13 | loss: 64.17\n",
      "epoch: 8/50 | batch: 3/13 | loss: 35.26\n",
      "epoch: 8/50 | batch: 4/13 | loss: 21.97\n",
      "epoch: 8/50 | batch: 5/13 | loss: 82.86\n",
      "epoch: 8/50 | batch: 6/13 | loss: 27.00\n",
      "epoch: 8/50 | batch: 7/13 | loss: 26.16\n",
      "epoch: 8/50 | batch: 8/13 | loss: 57.50\n",
      "epoch: 8/50 | batch: 9/13 | loss: 70.72\n",
      "epoch: 8/50 | batch: 10/13 | loss: 49.97\n",
      "epoch: 8/50 | batch: 11/13 | loss: 68.95\n",
      "epoch: 8/50 | batch: 12/13 | loss: 101.55\n",
      "epoch: 8/50 | batch: 13/13 | loss: 230.43\n",
      "epoch: 9/50 | batch: 1/13 | loss: 50.98\n",
      "epoch: 9/50 | batch: 2/13 | loss: 70.49\n",
      "epoch: 9/50 | batch: 3/13 | loss: 272.57\n",
      "epoch: 9/50 | batch: 4/13 | loss: 55.31\n",
      "epoch: 9/50 | batch: 5/13 | loss: 44.24\n",
      "epoch: 9/50 | batch: 6/13 | loss: 68.89\n",
      "epoch: 9/50 | batch: 7/13 | loss: 29.27\n",
      "epoch: 9/50 | batch: 8/13 | loss: 20.02\n",
      "epoch: 9/50 | batch: 9/13 | loss: 43.31\n",
      "epoch: 9/50 | batch: 10/13 | loss: 30.63\n",
      "epoch: 9/50 | batch: 11/13 | loss: 33.66\n",
      "epoch: 9/50 | batch: 12/13 | loss: 89.85\n",
      "epoch: 9/50 | batch: 13/13 | loss: 63.98\n",
      "epoch: 10/50 | batch: 1/13 | loss: 10.92\n",
      "epoch: 10/50 | batch: 2/13 | loss: 253.83\n",
      "epoch: 10/50 | batch: 3/13 | loss: 37.67\n",
      "epoch: 10/50 | batch: 4/13 | loss: 49.15\n",
      "epoch: 10/50 | batch: 5/13 | loss: 44.11\n",
      "epoch: 10/50 | batch: 6/13 | loss: 62.24\n",
      "epoch: 10/50 | batch: 7/13 | loss: 55.53\n",
      "epoch: 10/50 | batch: 8/13 | loss: 38.83\n",
      "epoch: 10/50 | batch: 9/13 | loss: 47.35\n",
      "epoch: 10/50 | batch: 10/13 | loss: 28.76\n",
      "epoch: 10/50 | batch: 11/13 | loss: 73.03\n",
      "epoch: 10/50 | batch: 12/13 | loss: 56.72\n",
      "epoch: 10/50 | batch: 13/13 | loss: 100.77\n",
      "epoch: 11/50 | batch: 1/13 | loss: 65.98\n",
      "epoch: 11/50 | batch: 2/13 | loss: 48.98\n",
      "epoch: 11/50 | batch: 3/13 | loss: 31.21\n",
      "epoch: 11/50 | batch: 4/13 | loss: 25.43\n",
      "epoch: 11/50 | batch: 5/13 | loss: 45.82\n",
      "epoch: 11/50 | batch: 6/13 | loss: 35.26\n",
      "epoch: 11/50 | batch: 7/13 | loss: 42.41\n",
      "epoch: 11/50 | batch: 8/13 | loss: 91.96\n",
      "epoch: 11/50 | batch: 9/13 | loss: 17.94\n",
      "epoch: 11/50 | batch: 10/13 | loss: 90.53\n",
      "epoch: 11/50 | batch: 11/13 | loss: 84.56\n",
      "epoch: 11/50 | batch: 12/13 | loss: 26.28\n",
      "epoch: 11/50 | batch: 13/13 | loss: 254.77\n",
      "epoch: 12/50 | batch: 1/13 | loss: 40.75\n",
      "epoch: 12/50 | batch: 2/13 | loss: 200.89\n",
      "epoch: 12/50 | batch: 3/13 | loss: 49.55\n",
      "epoch: 12/50 | batch: 4/13 | loss: 61.56\n",
      "epoch: 12/50 | batch: 5/13 | loss: 108.34\n",
      "epoch: 12/50 | batch: 6/13 | loss: 35.14\n",
      "epoch: 12/50 | batch: 7/13 | loss: 25.93\n",
      "epoch: 12/50 | batch: 8/13 | loss: 79.28\n",
      "epoch: 12/50 | batch: 9/13 | loss: 46.94\n",
      "epoch: 12/50 | batch: 10/13 | loss: 47.57\n",
      "epoch: 12/50 | batch: 11/13 | loss: 57.83\n",
      "epoch: 12/50 | batch: 12/13 | loss: 50.58\n",
      "epoch: 12/50 | batch: 13/13 | loss: 37.22\n",
      "epoch: 13/50 | batch: 1/13 | loss: 27.98\n",
      "epoch: 13/50 | batch: 2/13 | loss: 55.42\n",
      "epoch: 13/50 | batch: 3/13 | loss: 55.69\n",
      "epoch: 13/50 | batch: 4/13 | loss: 48.09\n",
      "epoch: 13/50 | batch: 5/13 | loss: 95.88\n",
      "epoch: 13/50 | batch: 6/13 | loss: 25.77\n",
      "epoch: 13/50 | batch: 7/13 | loss: 39.54\n",
      "epoch: 13/50 | batch: 8/13 | loss: 27.32\n",
      "epoch: 13/50 | batch: 9/13 | loss: 81.64\n",
      "epoch: 13/50 | batch: 10/13 | loss: 74.54\n",
      "epoch: 13/50 | batch: 11/13 | loss: 23.01\n",
      "epoch: 13/50 | batch: 12/13 | loss: 235.16\n",
      "epoch: 13/50 | batch: 13/13 | loss: 43.47\n",
      "epoch: 14/50 | batch: 1/13 | loss: 35.04\n",
      "epoch: 14/50 | batch: 2/13 | loss: 62.32\n",
      "epoch: 14/50 | batch: 3/13 | loss: 43.79\n",
      "epoch: 14/50 | batch: 4/13 | loss: 202.30\n",
      "epoch: 14/50 | batch: 5/13 | loss: 32.58\n",
      "epoch: 14/50 | batch: 6/13 | loss: 86.89\n",
      "epoch: 14/50 | batch: 7/13 | loss: 91.69\n",
      "epoch: 14/50 | batch: 8/13 | loss: 23.26\n",
      "epoch: 14/50 | batch: 9/13 | loss: 26.63\n",
      "epoch: 14/50 | batch: 10/13 | loss: 33.15\n",
      "epoch: 14/50 | batch: 11/13 | loss: 78.16\n",
      "epoch: 14/50 | batch: 12/13 | loss: 67.57\n",
      "epoch: 14/50 | batch: 13/13 | loss: 44.48\n",
      "epoch: 15/50 | batch: 1/13 | loss: 43.37\n",
      "epoch: 15/50 | batch: 2/13 | loss: 39.87\n",
      "epoch: 15/50 | batch: 3/13 | loss: 213.99\n",
      "epoch: 15/50 | batch: 4/13 | loss: 30.25\n",
      "epoch: 15/50 | batch: 5/13 | loss: 43.30\n",
      "epoch: 15/50 | batch: 6/13 | loss: 53.66\n",
      "epoch: 15/50 | batch: 7/13 | loss: 64.97\n",
      "epoch: 15/50 | batch: 8/13 | loss: 75.41\n",
      "epoch: 15/50 | batch: 9/13 | loss: 74.55\n",
      "epoch: 15/50 | batch: 10/13 | loss: 24.76\n",
      "epoch: 15/50 | batch: 11/13 | loss: 20.94\n",
      "epoch: 15/50 | batch: 12/13 | loss: 105.65\n",
      "epoch: 15/50 | batch: 13/13 | loss: 28.06\n",
      "epoch: 16/50 | batch: 1/13 | loss: 52.71\n",
      "epoch: 16/50 | batch: 2/13 | loss: 265.43\n",
      "epoch: 16/50 | batch: 3/13 | loss: 78.07\n",
      "epoch: 16/50 | batch: 4/13 | loss: 35.16\n",
      "epoch: 16/50 | batch: 5/13 | loss: 43.81\n",
      "epoch: 16/50 | batch: 6/13 | loss: 29.67\n",
      "epoch: 16/50 | batch: 7/13 | loss: 24.01\n",
      "epoch: 16/50 | batch: 8/13 | loss: 28.86\n",
      "epoch: 16/50 | batch: 9/13 | loss: 62.08\n",
      "epoch: 16/50 | batch: 10/13 | loss: 58.58\n",
      "epoch: 16/50 | batch: 11/13 | loss: 31.04\n",
      "epoch: 16/50 | batch: 12/13 | loss: 42.71\n",
      "epoch: 16/50 | batch: 13/13 | loss: 70.47\n",
      "epoch: 17/50 | batch: 1/13 | loss: 48.36\n",
      "epoch: 17/50 | batch: 2/13 | loss: 45.87\n",
      "epoch: 17/50 | batch: 3/13 | loss: 100.76\n",
      "epoch: 17/50 | batch: 4/13 | loss: 50.66\n",
      "epoch: 17/50 | batch: 5/13 | loss: 52.54\n",
      "epoch: 17/50 | batch: 6/13 | loss: 64.88\n",
      "epoch: 17/50 | batch: 7/13 | loss: 26.50\n",
      "epoch: 17/50 | batch: 8/13 | loss: 43.51\n",
      "epoch: 17/50 | batch: 9/13 | loss: 43.21\n",
      "epoch: 17/50 | batch: 10/13 | loss: 37.95\n",
      "epoch: 17/50 | batch: 11/13 | loss: 32.85\n",
      "epoch: 17/50 | batch: 12/13 | loss: 43.60\n",
      "epoch: 17/50 | batch: 13/13 | loss: 240.24\n",
      "epoch: 18/50 | batch: 1/13 | loss: 71.75\n",
      "epoch: 18/50 | batch: 2/13 | loss: 57.47\n",
      "epoch: 18/50 | batch: 3/13 | loss: 58.72\n",
      "epoch: 18/50 | batch: 4/13 | loss: 30.78\n",
      "epoch: 18/50 | batch: 5/13 | loss: 28.78\n",
      "epoch: 18/50 | batch: 6/13 | loss: 53.47\n",
      "epoch: 18/50 | batch: 7/13 | loss: 50.66\n",
      "epoch: 18/50 | batch: 8/13 | loss: 62.14\n",
      "epoch: 18/50 | batch: 9/13 | loss: 221.20\n",
      "epoch: 18/50 | batch: 10/13 | loss: 41.83\n",
      "epoch: 18/50 | batch: 11/13 | loss: 49.53\n",
      "epoch: 18/50 | batch: 12/13 | loss: 61.79\n",
      "epoch: 18/50 | batch: 13/13 | loss: 20.01\n",
      "epoch: 19/50 | batch: 1/13 | loss: 36.59\n",
      "epoch: 19/50 | batch: 2/13 | loss: 99.43\n",
      "epoch: 19/50 | batch: 3/13 | loss: 218.01\n",
      "epoch: 19/50 | batch: 4/13 | loss: 58.92\n",
      "epoch: 19/50 | batch: 5/13 | loss: 50.50\n",
      "epoch: 19/50 | batch: 6/13 | loss: 68.67\n",
      "epoch: 19/50 | batch: 7/13 | loss: 33.59\n",
      "epoch: 19/50 | batch: 8/13 | loss: 29.21\n",
      "epoch: 19/50 | batch: 9/13 | loss: 38.87\n",
      "epoch: 19/50 | batch: 10/13 | loss: 64.69\n",
      "epoch: 19/50 | batch: 11/13 | loss: 23.57\n",
      "epoch: 19/50 | batch: 12/13 | loss: 52.35\n",
      "epoch: 19/50 | batch: 13/13 | loss: 34.40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20/50 | batch: 1/13 | loss: 83.73\n",
      "epoch: 20/50 | batch: 2/13 | loss: 26.43\n",
      "epoch: 20/50 | batch: 3/13 | loss: 48.28\n",
      "epoch: 20/50 | batch: 4/13 | loss: 30.82\n",
      "epoch: 20/50 | batch: 5/13 | loss: 27.53\n",
      "epoch: 20/50 | batch: 6/13 | loss: 204.64\n",
      "epoch: 20/50 | batch: 7/13 | loss: 44.58\n",
      "epoch: 20/50 | batch: 8/13 | loss: 76.95\n",
      "epoch: 20/50 | batch: 9/13 | loss: 56.34\n",
      "epoch: 20/50 | batch: 10/13 | loss: 52.06\n",
      "epoch: 20/50 | batch: 11/13 | loss: 74.59\n",
      "epoch: 20/50 | batch: 12/13 | loss: 36.40\n",
      "epoch: 20/50 | batch: 13/13 | loss: 42.84\n",
      "epoch: 21/50 | batch: 1/13 | loss: 25.03\n",
      "epoch: 21/50 | batch: 2/13 | loss: 45.26\n",
      "epoch: 21/50 | batch: 3/13 | loss: 36.34\n",
      "epoch: 21/50 | batch: 4/13 | loss: 205.23\n",
      "epoch: 21/50 | batch: 5/13 | loss: 92.99\n",
      "epoch: 21/50 | batch: 6/13 | loss: 73.92\n",
      "epoch: 21/50 | batch: 7/13 | loss: 34.13\n",
      "epoch: 21/50 | batch: 8/13 | loss: 116.57\n",
      "epoch: 21/50 | batch: 9/13 | loss: 25.21\n",
      "epoch: 21/50 | batch: 10/13 | loss: 38.13\n",
      "epoch: 21/50 | batch: 11/13 | loss: 54.83\n",
      "epoch: 21/50 | batch: 12/13 | loss: 29.25\n",
      "epoch: 21/50 | batch: 13/13 | loss: 24.26\n",
      "epoch: 22/50 | batch: 1/13 | loss: 51.85\n",
      "epoch: 22/50 | batch: 2/13 | loss: 35.36\n",
      "epoch: 22/50 | batch: 3/13 | loss: 36.12\n",
      "epoch: 22/50 | batch: 4/13 | loss: 33.60\n",
      "epoch: 22/50 | batch: 5/13 | loss: 59.71\n",
      "epoch: 22/50 | batch: 6/13 | loss: 38.89\n",
      "epoch: 22/50 | batch: 7/13 | loss: 46.24\n",
      "epoch: 22/50 | batch: 8/13 | loss: 43.70\n",
      "epoch: 22/50 | batch: 9/13 | loss: 67.72\n",
      "epoch: 22/50 | batch: 10/13 | loss: 61.05\n",
      "epoch: 22/50 | batch: 11/13 | loss: 54.92\n",
      "epoch: 22/50 | batch: 12/13 | loss: 222.11\n",
      "epoch: 22/50 | batch: 13/13 | loss: 50.05\n",
      "epoch: 23/50 | batch: 1/13 | loss: 91.26\n",
      "epoch: 23/50 | batch: 2/13 | loss: 55.09\n",
      "epoch: 23/50 | batch: 3/13 | loss: 59.82\n",
      "epoch: 23/50 | batch: 4/13 | loss: 37.46\n",
      "epoch: 23/50 | batch: 5/13 | loss: 34.17\n",
      "epoch: 23/50 | batch: 6/13 | loss: 63.89\n",
      "epoch: 23/50 | batch: 7/13 | loss: 68.85\n",
      "epoch: 23/50 | batch: 8/13 | loss: 50.97\n",
      "epoch: 23/50 | batch: 9/13 | loss: 35.26\n",
      "epoch: 23/50 | batch: 10/13 | loss: 24.38\n",
      "epoch: 23/50 | batch: 11/13 | loss: 212.40\n",
      "epoch: 23/50 | batch: 12/13 | loss: 27.35\n",
      "epoch: 23/50 | batch: 13/13 | loss: 38.94\n",
      "epoch: 24/50 | batch: 1/13 | loss: 73.21\n",
      "epoch: 24/50 | batch: 2/13 | loss: 35.30\n",
      "epoch: 24/50 | batch: 3/13 | loss: 75.49\n",
      "epoch: 24/50 | batch: 4/13 | loss: 39.48\n",
      "epoch: 24/50 | batch: 5/13 | loss: 65.40\n",
      "epoch: 24/50 | batch: 6/13 | loss: 29.47\n",
      "epoch: 24/50 | batch: 7/13 | loss: 37.98\n",
      "epoch: 24/50 | batch: 8/13 | loss: 48.86\n",
      "epoch: 24/50 | batch: 9/13 | loss: 30.00\n",
      "epoch: 24/50 | batch: 10/13 | loss: 223.47\n",
      "epoch: 24/50 | batch: 11/13 | loss: 24.87\n",
      "epoch: 24/50 | batch: 12/13 | loss: 48.16\n",
      "epoch: 24/50 | batch: 13/13 | loss: 64.41\n",
      "epoch: 25/50 | batch: 1/13 | loss: 51.62\n",
      "epoch: 25/50 | batch: 2/13 | loss: 25.56\n",
      "epoch: 25/50 | batch: 3/13 | loss: 25.60\n",
      "epoch: 25/50 | batch: 4/13 | loss: 182.27\n",
      "epoch: 25/50 | batch: 5/13 | loss: 30.90\n",
      "epoch: 25/50 | batch: 6/13 | loss: 75.97\n",
      "epoch: 25/50 | batch: 7/13 | loss: 88.52\n",
      "epoch: 25/50 | batch: 8/13 | loss: 61.24\n",
      "epoch: 25/50 | batch: 9/13 | loss: 43.02\n",
      "epoch: 25/50 | batch: 10/13 | loss: 27.38\n",
      "epoch: 25/50 | batch: 11/13 | loss: 60.56\n",
      "epoch: 25/50 | batch: 12/13 | loss: 67.96\n",
      "epoch: 25/50 | batch: 13/13 | loss: 51.30\n",
      "epoch: 26/50 | batch: 1/13 | loss: 29.53\n",
      "epoch: 26/50 | batch: 2/13 | loss: 191.22\n",
      "epoch: 26/50 | batch: 3/13 | loss: 65.17\n",
      "epoch: 26/50 | batch: 4/13 | loss: 23.56\n",
      "epoch: 26/50 | batch: 5/13 | loss: 63.60\n",
      "epoch: 26/50 | batch: 6/13 | loss: 34.69\n",
      "epoch: 26/50 | batch: 7/13 | loss: 44.12\n",
      "epoch: 26/50 | batch: 8/13 | loss: 27.29\n",
      "epoch: 26/50 | batch: 9/13 | loss: 47.85\n",
      "epoch: 26/50 | batch: 10/13 | loss: 70.91\n",
      "epoch: 26/50 | batch: 11/13 | loss: 58.39\n",
      "epoch: 26/50 | batch: 12/13 | loss: 83.07\n",
      "epoch: 26/50 | batch: 13/13 | loss: 49.51\n",
      "epoch: 27/50 | batch: 1/13 | loss: 34.89\n",
      "epoch: 27/50 | batch: 2/13 | loss: 26.57\n",
      "epoch: 27/50 | batch: 3/13 | loss: 55.18\n",
      "epoch: 27/50 | batch: 4/13 | loss: 58.10\n",
      "epoch: 27/50 | batch: 5/13 | loss: 79.91\n",
      "epoch: 27/50 | batch: 6/13 | loss: 29.48\n",
      "epoch: 27/50 | batch: 7/13 | loss: 41.19\n",
      "epoch: 27/50 | batch: 8/13 | loss: 53.48\n",
      "epoch: 27/50 | batch: 9/13 | loss: 27.22\n",
      "epoch: 27/50 | batch: 10/13 | loss: 37.49\n",
      "epoch: 27/50 | batch: 11/13 | loss: 94.05\n",
      "epoch: 27/50 | batch: 12/13 | loss: 215.59\n",
      "epoch: 27/50 | batch: 13/13 | loss: 39.61\n",
      "epoch: 28/50 | batch: 1/13 | loss: 38.89\n",
      "epoch: 28/50 | batch: 2/13 | loss: 63.54\n",
      "epoch: 28/50 | batch: 3/13 | loss: 54.80\n",
      "epoch: 28/50 | batch: 4/13 | loss: 38.60\n",
      "epoch: 28/50 | batch: 5/13 | loss: 31.95\n",
      "epoch: 28/50 | batch: 6/13 | loss: 90.62\n",
      "epoch: 28/50 | batch: 7/13 | loss: 37.49\n",
      "epoch: 28/50 | batch: 8/13 | loss: 22.31\n",
      "epoch: 28/50 | batch: 9/13 | loss: 44.46\n",
      "epoch: 28/50 | batch: 10/13 | loss: 58.58\n",
      "epoch: 28/50 | batch: 11/13 | loss: 62.30\n",
      "epoch: 28/50 | batch: 12/13 | loss: 36.54\n",
      "epoch: 28/50 | batch: 13/13 | loss: 216.04\n",
      "epoch: 29/50 | batch: 1/13 | loss: 20.42\n",
      "epoch: 29/50 | batch: 2/13 | loss: 84.73\n",
      "epoch: 29/50 | batch: 3/13 | loss: 40.58\n",
      "epoch: 29/50 | batch: 4/13 | loss: 61.76\n",
      "epoch: 29/50 | batch: 5/13 | loss: 53.27\n",
      "epoch: 29/50 | batch: 6/13 | loss: 30.47\n",
      "epoch: 29/50 | batch: 7/13 | loss: 46.02\n",
      "epoch: 29/50 | batch: 8/13 | loss: 48.26\n",
      "epoch: 29/50 | batch: 9/13 | loss: 39.60\n",
      "epoch: 29/50 | batch: 10/13 | loss: 20.70\n",
      "epoch: 29/50 | batch: 11/13 | loss: 29.29\n",
      "epoch: 29/50 | batch: 12/13 | loss: 69.33\n",
      "epoch: 29/50 | batch: 13/13 | loss: 249.99\n",
      "epoch: 30/50 | batch: 1/13 | loss: 63.03\n",
      "epoch: 30/50 | batch: 2/13 | loss: 31.00\n",
      "epoch: 30/50 | batch: 3/13 | loss: 20.01\n",
      "epoch: 30/50 | batch: 4/13 | loss: 35.75\n",
      "epoch: 30/50 | batch: 5/13 | loss: 60.28\n",
      "epoch: 30/50 | batch: 6/13 | loss: 61.20\n",
      "epoch: 30/50 | batch: 7/13 | loss: 29.34\n",
      "epoch: 30/50 | batch: 8/13 | loss: 47.58\n",
      "epoch: 30/50 | batch: 9/13 | loss: 186.13\n",
      "epoch: 30/50 | batch: 10/13 | loss: 27.32\n",
      "epoch: 30/50 | batch: 11/13 | loss: 83.72\n",
      "epoch: 30/50 | batch: 12/13 | loss: 112.79\n",
      "epoch: 30/50 | batch: 13/13 | loss: 23.84\n",
      "epoch: 31/50 | batch: 1/13 | loss: 189.37\n",
      "epoch: 31/50 | batch: 2/13 | loss: 19.62\n",
      "epoch: 31/50 | batch: 3/13 | loss: 55.37\n",
      "epoch: 31/50 | batch: 4/13 | loss: 88.93\n",
      "epoch: 31/50 | batch: 5/13 | loss: 37.55\n",
      "epoch: 31/50 | batch: 6/13 | loss: 52.07\n",
      "epoch: 31/50 | batch: 7/13 | loss: 42.96\n",
      "epoch: 31/50 | batch: 8/13 | loss: 53.43\n",
      "epoch: 31/50 | batch: 9/13 | loss: 79.34\n",
      "epoch: 31/50 | batch: 10/13 | loss: 25.32\n",
      "epoch: 31/50 | batch: 11/13 | loss: 37.13\n",
      "epoch: 31/50 | batch: 12/13 | loss: 59.61\n",
      "epoch: 31/50 | batch: 13/13 | loss: 40.92\n",
      "epoch: 32/50 | batch: 1/13 | loss: 37.20\n",
      "epoch: 32/50 | batch: 2/13 | loss: 48.44\n",
      "epoch: 32/50 | batch: 3/13 | loss: 111.83\n",
      "epoch: 32/50 | batch: 4/13 | loss: 30.59\n",
      "epoch: 32/50 | batch: 5/13 | loss: 23.11\n",
      "epoch: 32/50 | batch: 6/13 | loss: 29.08\n",
      "epoch: 32/50 | batch: 7/13 | loss: 201.93\n",
      "epoch: 32/50 | batch: 8/13 | loss: 30.82\n",
      "epoch: 32/50 | batch: 9/13 | loss: 78.18\n",
      "epoch: 32/50 | batch: 10/13 | loss: 31.21\n",
      "epoch: 32/50 | batch: 11/13 | loss: 55.28\n",
      "epoch: 32/50 | batch: 12/13 | loss: 65.98\n",
      "epoch: 32/50 | batch: 13/13 | loss: 35.54\n",
      "epoch: 33/50 | batch: 1/13 | loss: 44.53\n",
      "epoch: 33/50 | batch: 2/13 | loss: 30.58\n",
      "epoch: 33/50 | batch: 3/13 | loss: 116.62\n",
      "epoch: 33/50 | batch: 4/13 | loss: 16.57\n",
      "epoch: 33/50 | batch: 5/13 | loss: 34.63\n",
      "epoch: 33/50 | batch: 6/13 | loss: 106.53\n",
      "epoch: 33/50 | batch: 7/13 | loss: 47.90\n",
      "epoch: 33/50 | batch: 8/13 | loss: 34.32\n",
      "epoch: 33/50 | batch: 9/13 | loss: 190.41\n",
      "epoch: 33/50 | batch: 10/13 | loss: 56.74\n",
      "epoch: 33/50 | batch: 11/13 | loss: 25.25\n",
      "epoch: 33/50 | batch: 12/13 | loss: 30.68\n",
      "epoch: 33/50 | batch: 13/13 | loss: 39.61\n",
      "epoch: 34/50 | batch: 1/13 | loss: 45.93\n",
      "epoch: 34/50 | batch: 2/13 | loss: 50.85\n",
      "epoch: 34/50 | batch: 3/13 | loss: 64.71\n",
      "epoch: 34/50 | batch: 4/13 | loss: 40.67\n",
      "epoch: 34/50 | batch: 5/13 | loss: 79.81\n",
      "epoch: 34/50 | batch: 6/13 | loss: 42.51\n",
      "epoch: 34/50 | batch: 7/13 | loss: 31.33\n",
      "epoch: 34/50 | batch: 8/13 | loss: 31.86\n",
      "epoch: 34/50 | batch: 9/13 | loss: 27.41\n",
      "epoch: 34/50 | batch: 10/13 | loss: 70.30\n",
      "epoch: 34/50 | batch: 11/13 | loss: 32.57\n",
      "epoch: 34/50 | batch: 12/13 | loss: 234.94\n",
      "epoch: 34/50 | batch: 13/13 | loss: 22.90\n",
      "epoch: 35/50 | batch: 1/13 | loss: 38.64\n",
      "epoch: 35/50 | batch: 2/13 | loss: 74.36\n",
      "epoch: 35/50 | batch: 3/13 | loss: 27.01\n",
      "epoch: 35/50 | batch: 4/13 | loss: 45.67\n",
      "epoch: 35/50 | batch: 5/13 | loss: 49.22\n",
      "epoch: 35/50 | batch: 6/13 | loss: 224.87\n",
      "epoch: 35/50 | batch: 7/13 | loss: 58.61\n",
      "epoch: 35/50 | batch: 8/13 | loss: 21.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 35/50 | batch: 9/13 | loss: 52.97\n",
      "epoch: 35/50 | batch: 10/13 | loss: 54.05\n",
      "epoch: 35/50 | batch: 11/13 | loss: 45.18\n",
      "epoch: 35/50 | batch: 12/13 | loss: 28.51\n",
      "epoch: 35/50 | batch: 13/13 | loss: 54.66\n",
      "epoch: 36/50 | batch: 1/13 | loss: 59.57\n",
      "epoch: 36/50 | batch: 2/13 | loss: 22.93\n",
      "epoch: 36/50 | batch: 3/13 | loss: 68.26\n",
      "epoch: 36/50 | batch: 4/13 | loss: 39.17\n",
      "epoch: 36/50 | batch: 5/13 | loss: 32.36\n",
      "epoch: 36/50 | batch: 6/13 | loss: 50.94\n",
      "epoch: 36/50 | batch: 7/13 | loss: 36.67\n",
      "epoch: 36/50 | batch: 8/13 | loss: 35.99\n",
      "epoch: 36/50 | batch: 9/13 | loss: 50.16\n",
      "epoch: 36/50 | batch: 10/13 | loss: 54.02\n",
      "epoch: 36/50 | batch: 11/13 | loss: 19.57\n",
      "epoch: 36/50 | batch: 12/13 | loss: 57.61\n",
      "epoch: 36/50 | batch: 13/13 | loss: 257.66\n",
      "epoch: 37/50 | batch: 1/13 | loss: 84.89\n",
      "epoch: 37/50 | batch: 2/13 | loss: 185.35\n",
      "epoch: 37/50 | batch: 3/13 | loss: 32.01\n",
      "epoch: 37/50 | batch: 4/13 | loss: 24.76\n",
      "epoch: 37/50 | batch: 5/13 | loss: 29.99\n",
      "epoch: 37/50 | batch: 6/13 | loss: 47.04\n",
      "epoch: 37/50 | batch: 7/13 | loss: 29.94\n",
      "epoch: 37/50 | batch: 8/13 | loss: 45.94\n",
      "epoch: 37/50 | batch: 9/13 | loss: 52.57\n",
      "epoch: 37/50 | batch: 10/13 | loss: 88.47\n",
      "epoch: 37/50 | batch: 11/13 | loss: 42.73\n",
      "epoch: 37/50 | batch: 12/13 | loss: 56.89\n",
      "epoch: 37/50 | batch: 13/13 | loss: 52.82\n",
      "epoch: 38/50 | batch: 1/13 | loss: 64.08\n",
      "epoch: 38/50 | batch: 2/13 | loss: 20.17\n",
      "epoch: 38/50 | batch: 3/13 | loss: 209.92\n",
      "epoch: 38/50 | batch: 4/13 | loss: 38.43\n",
      "epoch: 38/50 | batch: 5/13 | loss: 90.16\n",
      "epoch: 38/50 | batch: 6/13 | loss: 43.33\n",
      "epoch: 38/50 | batch: 7/13 | loss: 87.51\n",
      "epoch: 38/50 | batch: 8/13 | loss: 48.97\n",
      "epoch: 38/50 | batch: 9/13 | loss: 37.47\n",
      "epoch: 38/50 | batch: 10/13 | loss: 13.36\n",
      "epoch: 38/50 | batch: 11/13 | loss: 56.26\n",
      "epoch: 38/50 | batch: 12/13 | loss: 26.78\n",
      "epoch: 38/50 | batch: 13/13 | loss: 38.02\n",
      "epoch: 39/50 | batch: 1/13 | loss: 42.29\n",
      "epoch: 39/50 | batch: 2/13 | loss: 234.67\n",
      "epoch: 39/50 | batch: 3/13 | loss: 28.29\n",
      "epoch: 39/50 | batch: 4/13 | loss: 63.70\n",
      "epoch: 39/50 | batch: 5/13 | loss: 37.98\n",
      "epoch: 39/50 | batch: 6/13 | loss: 28.05\n",
      "epoch: 39/50 | batch: 7/13 | loss: 42.07\n",
      "epoch: 39/50 | batch: 8/13 | loss: 63.87\n",
      "epoch: 39/50 | batch: 9/13 | loss: 45.49\n",
      "epoch: 39/50 | batch: 10/13 | loss: 34.49\n",
      "epoch: 39/50 | batch: 11/13 | loss: 96.50\n",
      "epoch: 39/50 | batch: 12/13 | loss: 34.60\n",
      "epoch: 39/50 | batch: 13/13 | loss: 17.96\n",
      "epoch: 40/50 | batch: 1/13 | loss: 22.36\n",
      "epoch: 40/50 | batch: 2/13 | loss: 40.25\n",
      "epoch: 40/50 | batch: 3/13 | loss: 94.81\n",
      "epoch: 40/50 | batch: 4/13 | loss: 19.35\n",
      "epoch: 40/50 | batch: 5/13 | loss: 66.15\n",
      "epoch: 40/50 | batch: 6/13 | loss: 107.44\n",
      "epoch: 40/50 | batch: 7/13 | loss: 192.57\n",
      "epoch: 40/50 | batch: 8/13 | loss: 23.05\n",
      "epoch: 40/50 | batch: 9/13 | loss: 51.83\n",
      "epoch: 40/50 | batch: 10/13 | loss: 14.93\n",
      "epoch: 40/50 | batch: 11/13 | loss: 41.64\n",
      "epoch: 40/50 | batch: 12/13 | loss: 42.69\n",
      "epoch: 40/50 | batch: 13/13 | loss: 54.45\n",
      "epoch: 41/50 | batch: 1/13 | loss: 81.69\n",
      "epoch: 41/50 | batch: 2/13 | loss: 48.64\n",
      "epoch: 41/50 | batch: 3/13 | loss: 18.13\n",
      "epoch: 41/50 | batch: 4/13 | loss: 39.87\n",
      "epoch: 41/50 | batch: 5/13 | loss: 27.48\n",
      "epoch: 41/50 | batch: 6/13 | loss: 25.02\n",
      "epoch: 41/50 | batch: 7/13 | loss: 43.56\n",
      "epoch: 41/50 | batch: 8/13 | loss: 86.54\n",
      "epoch: 41/50 | batch: 9/13 | loss: 54.42\n",
      "epoch: 41/50 | batch: 10/13 | loss: 79.60\n",
      "epoch: 41/50 | batch: 11/13 | loss: 18.77\n",
      "epoch: 41/50 | batch: 12/13 | loss: 31.20\n",
      "epoch: 41/50 | batch: 13/13 | loss: 223.89\n",
      "epoch: 42/50 | batch: 1/13 | loss: 29.87\n",
      "epoch: 42/50 | batch: 2/13 | loss: 212.13\n",
      "epoch: 42/50 | batch: 3/13 | loss: 35.05\n",
      "epoch: 42/50 | batch: 4/13 | loss: 55.19\n",
      "epoch: 42/50 | batch: 5/13 | loss: 81.50\n",
      "epoch: 42/50 | batch: 6/13 | loss: 28.96\n",
      "epoch: 42/50 | batch: 7/13 | loss: 51.19\n",
      "epoch: 42/50 | batch: 8/13 | loss: 49.57\n",
      "epoch: 42/50 | batch: 9/13 | loss: 48.96\n",
      "epoch: 42/50 | batch: 10/13 | loss: 43.32\n",
      "epoch: 42/50 | batch: 11/13 | loss: 58.07\n",
      "epoch: 42/50 | batch: 12/13 | loss: 42.78\n",
      "epoch: 42/50 | batch: 13/13 | loss: 26.19\n",
      "epoch: 43/50 | batch: 1/13 | loss: 86.32\n",
      "epoch: 43/50 | batch: 2/13 | loss: 34.58\n",
      "epoch: 43/50 | batch: 3/13 | loss: 32.12\n",
      "epoch: 43/50 | batch: 4/13 | loss: 15.53\n",
      "epoch: 43/50 | batch: 5/13 | loss: 31.04\n",
      "epoch: 43/50 | batch: 6/13 | loss: 62.25\n",
      "epoch: 43/50 | batch: 7/13 | loss: 60.20\n",
      "epoch: 43/50 | batch: 8/13 | loss: 48.75\n",
      "epoch: 43/50 | batch: 9/13 | loss: 227.33\n",
      "epoch: 43/50 | batch: 10/13 | loss: 71.33\n",
      "epoch: 43/50 | batch: 11/13 | loss: 21.46\n",
      "epoch: 43/50 | batch: 12/13 | loss: 30.48\n",
      "epoch: 43/50 | batch: 13/13 | loss: 43.75\n",
      "epoch: 44/50 | batch: 1/13 | loss: 58.63\n",
      "epoch: 44/50 | batch: 2/13 | loss: 47.44\n",
      "epoch: 44/50 | batch: 3/13 | loss: 37.71\n",
      "epoch: 44/50 | batch: 4/13 | loss: 36.91\n",
      "epoch: 44/50 | batch: 5/13 | loss: 23.18\n",
      "epoch: 44/50 | batch: 6/13 | loss: 42.18\n",
      "epoch: 44/50 | batch: 7/13 | loss: 35.42\n",
      "epoch: 44/50 | batch: 8/13 | loss: 34.56\n",
      "epoch: 44/50 | batch: 9/13 | loss: 40.36\n",
      "epoch: 44/50 | batch: 10/13 | loss: 228.56\n",
      "epoch: 44/50 | batch: 11/13 | loss: 89.26\n",
      "epoch: 44/50 | batch: 12/13 | loss: 33.86\n",
      "epoch: 44/50 | batch: 13/13 | loss: 55.97\n",
      "epoch: 45/50 | batch: 1/13 | loss: 41.74\n",
      "epoch: 45/50 | batch: 2/13 | loss: 33.56\n",
      "epoch: 45/50 | batch: 3/13 | loss: 27.81\n",
      "epoch: 45/50 | batch: 4/13 | loss: 39.62\n",
      "epoch: 45/50 | batch: 5/13 | loss: 49.28\n",
      "epoch: 45/50 | batch: 6/13 | loss: 29.17\n",
      "epoch: 45/50 | batch: 7/13 | loss: 59.53\n",
      "epoch: 45/50 | batch: 8/13 | loss: 64.49\n",
      "epoch: 45/50 | batch: 9/13 | loss: 54.76\n",
      "epoch: 45/50 | batch: 10/13 | loss: 28.41\n",
      "epoch: 45/50 | batch: 11/13 | loss: 187.98\n",
      "epoch: 45/50 | batch: 12/13 | loss: 89.13\n",
      "epoch: 45/50 | batch: 13/13 | loss: 54.51\n",
      "epoch: 46/50 | batch: 1/13 | loss: 33.63\n",
      "epoch: 46/50 | batch: 2/13 | loss: 52.66\n",
      "epoch: 46/50 | batch: 3/13 | loss: 39.16\n",
      "epoch: 46/50 | batch: 4/13 | loss: 50.15\n",
      "epoch: 46/50 | batch: 5/13 | loss: 25.22\n",
      "epoch: 46/50 | batch: 6/13 | loss: 95.69\n",
      "epoch: 46/50 | batch: 7/13 | loss: 49.06\n",
      "epoch: 46/50 | batch: 8/13 | loss: 43.63\n",
      "epoch: 46/50 | batch: 9/13 | loss: 13.58\n",
      "epoch: 46/50 | batch: 10/13 | loss: 48.33\n",
      "epoch: 46/50 | batch: 11/13 | loss: 73.14\n",
      "epoch: 46/50 | batch: 12/13 | loss: 25.07\n",
      "epoch: 46/50 | batch: 13/13 | loss: 224.10\n",
      "epoch: 47/50 | batch: 1/13 | loss: 41.33\n",
      "epoch: 47/50 | batch: 2/13 | loss: 49.63\n",
      "epoch: 47/50 | batch: 3/13 | loss: 45.67\n",
      "epoch: 47/50 | batch: 4/13 | loss: 61.54\n",
      "epoch: 47/50 | batch: 5/13 | loss: 60.34\n",
      "epoch: 47/50 | batch: 6/13 | loss: 40.55\n",
      "epoch: 47/50 | batch: 7/13 | loss: 40.79\n",
      "epoch: 47/50 | batch: 8/13 | loss: 21.02\n",
      "epoch: 47/50 | batch: 9/13 | loss: 214.56\n",
      "epoch: 47/50 | batch: 10/13 | loss: 27.39\n",
      "epoch: 47/50 | batch: 11/13 | loss: 86.25\n",
      "epoch: 47/50 | batch: 12/13 | loss: 38.08\n",
      "epoch: 47/50 | batch: 13/13 | loss: 34.10\n",
      "epoch: 48/50 | batch: 1/13 | loss: 200.28\n",
      "epoch: 48/50 | batch: 2/13 | loss: 29.98\n",
      "epoch: 48/50 | batch: 3/13 | loss: 50.74\n",
      "epoch: 48/50 | batch: 4/13 | loss: 89.41\n",
      "epoch: 48/50 | batch: 5/13 | loss: 43.57\n",
      "epoch: 48/50 | batch: 6/13 | loss: 87.39\n",
      "epoch: 48/50 | batch: 7/13 | loss: 32.65\n",
      "epoch: 48/50 | batch: 8/13 | loss: 53.33\n",
      "epoch: 48/50 | batch: 9/13 | loss: 44.26\n",
      "epoch: 48/50 | batch: 10/13 | loss: 48.55\n",
      "epoch: 48/50 | batch: 11/13 | loss: 24.36\n",
      "epoch: 48/50 | batch: 12/13 | loss: 20.76\n",
      "epoch: 48/50 | batch: 13/13 | loss: 31.87\n",
      "epoch: 49/50 | batch: 1/13 | loss: 57.74\n",
      "epoch: 49/50 | batch: 2/13 | loss: 224.06\n",
      "epoch: 49/50 | batch: 3/13 | loss: 64.87\n",
      "epoch: 49/50 | batch: 4/13 | loss: 42.74\n",
      "epoch: 49/50 | batch: 5/13 | loss: 14.25\n",
      "epoch: 49/50 | batch: 6/13 | loss: 29.30\n",
      "epoch: 49/50 | batch: 7/13 | loss: 82.93\n",
      "epoch: 49/50 | batch: 8/13 | loss: 38.64\n",
      "epoch: 49/50 | batch: 9/13 | loss: 37.28\n",
      "epoch: 49/50 | batch: 10/13 | loss: 31.79\n",
      "epoch: 49/50 | batch: 11/13 | loss: 28.71\n",
      "epoch: 49/50 | batch: 12/13 | loss: 38.94\n",
      "epoch: 49/50 | batch: 13/13 | loss: 71.53\n",
      "epoch: 50/50 | batch: 1/13 | loss: 25.85\n",
      "epoch: 50/50 | batch: 2/13 | loss: 37.85\n",
      "epoch: 50/50 | batch: 3/13 | loss: 37.88\n",
      "epoch: 50/50 | batch: 4/13 | loss: 195.93\n",
      "epoch: 50/50 | batch: 5/13 | loss: 45.25\n",
      "epoch: 50/50 | batch: 6/13 | loss: 62.59\n",
      "epoch: 50/50 | batch: 7/13 | loss: 33.04\n",
      "epoch: 50/50 | batch: 8/13 | loss: 96.73\n",
      "epoch: 50/50 | batch: 9/13 | loss: 62.74\n",
      "epoch: 50/50 | batch: 10/13 | loss: 45.94\n",
      "epoch: 50/50 | batch: 11/13 | loss: 22.05\n",
      "epoch: 50/50 | batch: 12/13 | loss: 36.00\n",
      "epoch: 50/50 | batch: 13/13 | loss: 58.36\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "loss_history = []\n",
    "print('='*50)\n",
    "for _ in range(epochs):\n",
    "    numBatch = np.round(x_train.shape[0]/batch_size)\n",
    "    batch = 1\n",
    "    for data in dl:\n",
    "        x, y = data\n",
    "        opt.zero_grad()\n",
    "        predicted = model(x)\n",
    "        loss_value = loss_func(predicted, y.unsqueeze(1))\n",
    "        loss_value.backward()\n",
    "        opt.step()\n",
    "        loss_history.append(loss_value)\n",
    "        print(f'epoch: {_+1}/{int(epochs)} | batch: {batch}/{int(numBatch)} | loss: {loss_value:.2f}')\n",
    "        batch += 1\n",
    "print('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "52b6b28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([47.0625], grad_fn=<AddBackward0>)\n",
      "tensor(37.9000)\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "print(model(x_train[idx]))\n",
    "print(y_train[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "96430594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(414,)\n",
      "<class 'numpy.ndarray'>\n",
      "(414,)\n"
     ]
    }
   ],
   "source": [
    "output = model(x_train).detach().numpy().squeeze()\n",
    "print(type(output))\n",
    "print(output.shape)\n",
    "target = y_train.detach().numpy()\n",
    "print(type(target))\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "5c2fde59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(37.9, 47.062515),\n",
       " (42.2, 44.978695),\n",
       " (47.3, 50.542786),\n",
       " (54.8, 49.732452),\n",
       " (43.1, 48.425217),\n",
       " (32.1, 27.957642),\n",
       " (40.3, 37.594604),\n",
       " (46.7, 46.76162),\n",
       " (18.8, 19.051046),\n",
       " (22.1, 25.862442),\n",
       " (41.4, 36.364967),\n",
       " (58.1, 54.94413),\n",
       " (39.3, 38.308365),\n",
       " (23.8, 20.542212),\n",
       " (34.3, 43.170784),\n",
       " (50.5, 44.131157),\n",
       " (70.1, 54.800606),\n",
       " (37.4, 37.22494),\n",
       " (42.3, 44.23482),\n",
       " (47.7, 50.811253),\n",
       " (29.3, 29.589682),\n",
       " (51.6, 50.727062),\n",
       " (24.6, 24.299232),\n",
       " (47.9, 48.221596),\n",
       " (38.8, 37.647675),\n",
       " (27.0, 27.350067),\n",
       " (56.2, 49.646465),\n",
       " (33.6, 41.151127),\n",
       " (47.0, 40.871883),\n",
       " (57.1, 47.196945),\n",
       " (22.1, 19.829758),\n",
       " (25.0, 36.994495),\n",
       " (34.2, 33.56148),\n",
       " (49.3, 45.927437),\n",
       " (55.1, 47.10122),\n",
       " (27.3, 31.284042),\n",
       " (22.9, 24.730268),\n",
       " (25.3, 26.47555),\n",
       " (47.7, 45.626266),\n",
       " (46.2, 46.558235),\n",
       " (15.9, 17.970406),\n",
       " (18.2, 21.097668),\n",
       " (34.7, 36.30705),\n",
       " (34.1, 41.67021),\n",
       " (53.9, 52.01875),\n",
       " (38.3, 39.677795),\n",
       " (42.0, 43.02918),\n",
       " (61.5, 40.30201),\n",
       " (13.4, 19.166008),\n",
       " (13.2, 14.636532),\n",
       " (44.2, 39.815132),\n",
       " (20.7, 19.468063),\n",
       " (27.0, 30.735895),\n",
       " (38.9, 38.873825),\n",
       " (51.7, 46.045113),\n",
       " (13.7, 19.649797),\n",
       " (41.9, 43.3091),\n",
       " (53.5, 46.356133),\n",
       " (22.6, 19.062412),\n",
       " (42.4, 38.18299),\n",
       " (21.3, 26.982235),\n",
       " (63.2, 53.915585),\n",
       " (27.7, 22.593262),\n",
       " (55.0, 52.07618),\n",
       " (25.3, 31.915998),\n",
       " (44.3, 47.927376),\n",
       " (50.7, 49.297264),\n",
       " (56.8, 49.800716),\n",
       " (36.2, 43.212067),\n",
       " (42.0, 45.312344),\n",
       " (59.0, 56.81917),\n",
       " (40.8, 36.221115),\n",
       " (36.3, 45.03439),\n",
       " (20.0, 19.123653),\n",
       " (54.4, 51.30134),\n",
       " (29.5, 28.8093),\n",
       " (36.8, 41.208973),\n",
       " (25.6, 21.150242),\n",
       " (29.8, 35.466072),\n",
       " (26.5, 22.554665),\n",
       " (40.3, 46.033474),\n",
       " (36.8, 35.581406),\n",
       " (48.1, 44.946903),\n",
       " (17.7, 18.731176),\n",
       " (43.7, 41.86387),\n",
       " (50.8, 50.109734),\n",
       " (27.0, 30.131264),\n",
       " (18.3, 21.701992),\n",
       " (48.0, 36.621887),\n",
       " (25.3, 20.122025),\n",
       " (45.4, 48.59895),\n",
       " (43.2, 38.785904),\n",
       " (21.8, 20.154446),\n",
       " (16.1, 19.783756),\n",
       " (41.0, 38.23434),\n",
       " (51.8, 45.91201),\n",
       " (59.5, 55.574554),\n",
       " (34.6, 36.41829),\n",
       " (51.0, 48.49244),\n",
       " (62.2, 55.574554),\n",
       " (38.2, 43.012405),\n",
       " (32.9, 41.28128),\n",
       " (54.4, 49.755955),\n",
       " (45.7, 45.61457),\n",
       " (30.5, 34.66948),\n",
       " (71.0, 52.352673),\n",
       " (47.1, 46.532135),\n",
       " (26.6, 27.553843),\n",
       " (34.1, 37.530872),\n",
       " (28.4, 31.00861),\n",
       " (51.6, 46.735695),\n",
       " (39.4, 47.09485),\n",
       " (23.1, 26.483078),\n",
       " (7.6, 40.259277),\n",
       " (53.3, 43.327602),\n",
       " (46.4, 38.466034),\n",
       " (12.2, 18.25716),\n",
       " (13.0, 18.123384),\n",
       " (30.6, 26.480675),\n",
       " (59.6, 49.168167),\n",
       " (31.3, 39.38829),\n",
       " (48.0, 41.855938),\n",
       " (32.5, 42.98179),\n",
       " (45.5, 51.83671),\n",
       " (57.4, 47.362766),\n",
       " (48.6, 50.38355),\n",
       " (62.9, 36.51326),\n",
       " (55.0, 52.209442),\n",
       " (60.7, 40.448364),\n",
       " (41.0, 47.62559),\n",
       " (37.5, 42.73561),\n",
       " (30.7, 30.773312),\n",
       " (37.5, 38.304653),\n",
       " (39.5, 45.26992),\n",
       " (42.2, 37.12596),\n",
       " (20.8, 28.649548),\n",
       " (46.8, 44.700584),\n",
       " (47.4, 44.778694),\n",
       " (43.5, 39.219543),\n",
       " (42.5, 37.991188),\n",
       " (51.4, 47.20492),\n",
       " (28.9, 34.880844),\n",
       " (37.5, 39.348816),\n",
       " (40.1, 41.855938),\n",
       " (28.4, 26.053951),\n",
       " (45.5, 49.025974),\n",
       " (52.2, 48.095516),\n",
       " (43.2, 46.883602),\n",
       " (45.1, 21.894985),\n",
       " (39.7, 40.81921),\n",
       " (48.5, 41.571266),\n",
       " (44.7, 55.44526),\n",
       " (28.9, 27.658657),\n",
       " (40.9, 41.947083),\n",
       " (20.7, 21.084435),\n",
       " (15.6, 19.123653),\n",
       " (18.3, 23.831936),\n",
       " (35.6, 38.735504),\n",
       " (39.4, 45.696926),\n",
       " (37.4, 36.196182),\n",
       " (57.8, 47.278336),\n",
       " (39.6, 40.186577),\n",
       " (11.6, 16.256002),\n",
       " (55.5, 49.800716),\n",
       " (55.2, 48.377228),\n",
       " (30.6, 33.814297),\n",
       " (73.6, 56.31253),\n",
       " (43.4, 43.537415),\n",
       " (37.4, 33.854786),\n",
       " (23.5, 25.473568),\n",
       " (14.4, 18.418507),\n",
       " (58.8, 51.13408),\n",
       " (58.1, 56.81917),\n",
       " (35.1, 43.014782),\n",
       " (45.2, 53.19468),\n",
       " (36.5, 35.44757),\n",
       " (19.2, 16.530668),\n",
       " (42.0, 44.915367),\n",
       " (36.7, 43.507492),\n",
       " (42.6, 39.836353),\n",
       " (15.5, 14.72237),\n",
       " (55.9, 52.459106),\n",
       " (23.6, 25.03732),\n",
       " (18.8, 21.177864),\n",
       " (21.8, 16.811964),\n",
       " (21.5, 25.061543),\n",
       " (25.7, 21.901499),\n",
       " (22.0, 15.855595),\n",
       " (44.3, 43.14073),\n",
       " (20.5, 16.826044),\n",
       " (42.3, 44.09719),\n",
       " (37.8, 39.597076),\n",
       " (42.7, 42.827576),\n",
       " (49.3, 50.726303),\n",
       " (29.3, 22.036686),\n",
       " (34.6, 36.946793),\n",
       " (36.6, 37.689827),\n",
       " (48.2, 42.946365),\n",
       " (39.1, 39.87085),\n",
       " (31.6, 43.374516),\n",
       " (25.5, 32.62625),\n",
       " (45.9, 49.054527),\n",
       " (31.5, 35.51169),\n",
       " (46.1, 44.26957),\n",
       " (26.6, 22.554665),\n",
       " (21.4, 30.086693),\n",
       " (44.0, 46.79027),\n",
       " (34.2, 36.151485),\n",
       " (26.2, 25.503193),\n",
       " (40.9, 41.45471),\n",
       " (52.2, 52.979378),\n",
       " (43.5, 49.945007),\n",
       " (31.1, 32.857815),\n",
       " (58.0, 52.97239),\n",
       " (20.9, 26.863201),\n",
       " (48.1, 43.890606),\n",
       " (39.7, 35.630844),\n",
       " (40.8, 39.76801),\n",
       " (43.8, 41.09687),\n",
       " (40.2, 39.362213),\n",
       " (78.3, 47.319935),\n",
       " (38.5, 39.391098),\n",
       " (48.5, 48.030373),\n",
       " (42.3, 38.785904),\n",
       " (46.0, 43.809547),\n",
       " (49.0, 51.00367),\n",
       " (12.8, 17.5238),\n",
       " (40.2, 43.118263),\n",
       " (46.6, 31.520094),\n",
       " (19.0, 24.602856),\n",
       " (33.4, 30.773312),\n",
       " (14.7, 16.2608),\n",
       " (17.4, 14.79421),\n",
       " (32.4, 47.14747),\n",
       " (23.9, 26.968843),\n",
       " (39.3, 42.768456),\n",
       " (61.9, 54.92798),\n",
       " (39.0, 38.8232),\n",
       " (40.6, 38.39892),\n",
       " (29.7, 34.65912),\n",
       " (28.8, 27.452574),\n",
       " (41.4, 46.53451),\n",
       " (33.4, 31.124489),\n",
       " (48.2, 48.091736),\n",
       " (21.7, 33.735382),\n",
       " (40.8, 46.518574),\n",
       " (40.6, 41.58052),\n",
       " (23.1, 29.926926),\n",
       " (22.3, 28.87074),\n",
       " (15.0, 16.392796),\n",
       " (30.0, 41.362316),\n",
       " (13.8, 19.852448),\n",
       " (52.7, 51.655),\n",
       " (25.9, 20.652382),\n",
       " (51.8, 51.863396),\n",
       " (17.4, 18.531155),\n",
       " (26.5, 38.652107),\n",
       " (43.9, 39.07892),\n",
       " (63.3, 56.31253),\n",
       " (28.8, 31.443882),\n",
       " (30.7, 28.32838),\n",
       " (24.4, 22.375341),\n",
       " (53.0, 45.057594),\n",
       " (31.7, 30.608791),\n",
       " (40.6, 38.880543),\n",
       " (38.1, 36.351242),\n",
       " (23.7, 24.930256),\n",
       " (41.1, 36.856163),\n",
       " (40.1, 45.922077),\n",
       " (23.0, 30.928703),\n",
       " (117.5, 45.550674),\n",
       " (26.5, 40.75116),\n",
       " (40.5, 38.068718),\n",
       " (29.3, 45.322514),\n",
       " (41.0, 39.80915),\n",
       " (49.7, 50.811253),\n",
       " (34.0, 33.748318),\n",
       " (27.7, 22.463655),\n",
       " (44.0, 45.61457),\n",
       " (31.1, 36.447662),\n",
       " (45.4, 50.437416),\n",
       " (44.8, 54.20051),\n",
       " (25.6, 30.188477),\n",
       " (23.5, 30.55212),\n",
       " (34.4, 41.274445),\n",
       " (55.3, 35.762337),\n",
       " (56.3, 52.039516),\n",
       " (32.9, 33.708668),\n",
       " (51.0, 48.680206),\n",
       " (44.5, 49.043694),\n",
       " (37.0, 35.003685),\n",
       " (54.4, 46.129112),\n",
       " (24.5, 35.562733),\n",
       " (42.5, 41.913544),\n",
       " (38.1, 45.46218),\n",
       " (21.8, 22.608307),\n",
       " (34.1, 38.28552),\n",
       " (28.5, 34.784626),\n",
       " (16.7, 19.975006),\n",
       " (46.1, 48.396744),\n",
       " (36.9, 48.97495),\n",
       " (35.7, 32.775608),\n",
       " (23.2, 22.917988),\n",
       " (38.4, 38.515835),\n",
       " (29.4, 26.312214),\n",
       " (55.0, 45.99369),\n",
       " (50.2, 45.253685),\n",
       " (24.7, 16.067507),\n",
       " (53.0, 48.49244),\n",
       " (19.1, 21.04212),\n",
       " (24.7, 25.793633),\n",
       " (42.2, 37.855408),\n",
       " (78.0, 45.363262),\n",
       " (42.8, 48.392265),\n",
       " (41.6, 48.048798),\n",
       " (27.3, 30.137281),\n",
       " (42.0, 44.7109),\n",
       " (37.5, 35.52668),\n",
       " (49.8, 54.43541),\n",
       " (26.9, 34.73533),\n",
       " (18.6, 17.584484),\n",
       " (37.7, 37.987507),\n",
       " (33.1, 42.247955),\n",
       " (42.5, 44.95128),\n",
       " (31.3, 26.741304),\n",
       " (38.1, 39.677795),\n",
       " (62.1, 50.399673),\n",
       " (36.7, 47.786587),\n",
       " (23.6, 26.821047),\n",
       " (19.2, 18.123384),\n",
       " (12.8, 20.702589),\n",
       " (15.6, 18.331303),\n",
       " (39.6, 36.666473),\n",
       " (38.4, 45.461308),\n",
       " (22.8, 37.308315),\n",
       " (36.5, 40.191288),\n",
       " (35.6, 32.913357),\n",
       " (30.9, 34.35675),\n",
       " (36.3, 40.03195),\n",
       " (50.4, 55.310207),\n",
       " (42.9, 38.81552),\n",
       " (37.0, 38.565613),\n",
       " (53.5, 51.207638),\n",
       " (46.6, 41.487846),\n",
       " (41.2, 28.091125),\n",
       " (37.9, 47.81381),\n",
       " (30.8, 26.950783),\n",
       " (11.2, 18.923096),\n",
       " (53.7, 49.809906),\n",
       " (47.0, 45.461308),\n",
       " (42.3, 38.537113),\n",
       " (28.6, 29.460093),\n",
       " (25.7, 20.272367),\n",
       " (31.3, 30.70253),\n",
       " (30.1, 28.211689),\n",
       " (60.7, 52.209442),\n",
       " (45.3, 42.509155),\n",
       " (44.9, 54.289154),\n",
       " (45.1, 50.38355),\n",
       " (24.7, 23.95895),\n",
       " (47.1, 45.573685),\n",
       " (63.3, 42.87731),\n",
       " (40.0, 42.081825),\n",
       " (48.0, 50.97),\n",
       " (33.1, 42.635223),\n",
       " (29.5, 21.981295),\n",
       " (24.8, 20.360798),\n",
       " (20.9, 24.927786),\n",
       " (43.1, 41.46513),\n",
       " (22.8, 21.48565),\n",
       " (42.1, 44.38553),\n",
       " (51.7, 46.3908),\n",
       " (41.5, 39.346664),\n",
       " (52.2, 49.945007),\n",
       " (49.5, 50.58715),\n",
       " (23.8, 28.235264),\n",
       " (30.5, 26.432648),\n",
       " (56.8, 49.898552),\n",
       " (37.4, 41.17251),\n",
       " (69.7, 55.54432),\n",
       " (53.3, 48.94084),\n",
       " (47.3, 56.89681),\n",
       " (29.3, 18.52414),\n",
       " (40.3, 36.3831),\n",
       " (12.9, 16.222876),\n",
       " (46.6, 49.88524),\n",
       " (55.3, 48.377228),\n",
       " (25.6, 23.271542),\n",
       " (27.3, 25.599567),\n",
       " (67.7, 43.478157),\n",
       " (38.6, 44.463295),\n",
       " (31.3, 27.11406),\n",
       " (35.3, 42.059425),\n",
       " (40.3, 38.522995),\n",
       " (24.7, 18.819244),\n",
       " (42.5, 36.921253),\n",
       " (31.9, 30.82447),\n",
       " (32.2, 42.274017),\n",
       " (23.0, 26.432648),\n",
       " (37.3, 41.876514),\n",
       " (35.5, 38.97181),\n",
       " (27.7, 27.492672),\n",
       " (28.5, 41.145935),\n",
       " (39.7, 44.232555),\n",
       " (41.2, 47.798138),\n",
       " (37.2, 37.02097),\n",
       " (40.5, 50.709),\n",
       " (22.3, 22.104353),\n",
       " (28.1, 23.228928),\n",
       " (15.4, 17.943409),\n",
       " (50.0, 51.2615),\n",
       " (40.6, 44.585476),\n",
       " (52.5, 46.200905),\n",
       " (63.9, 56.19686)]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = list(zip(target, output))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "99b69b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6881874737483678"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2 = r2_score(target, output)\n",
    "r2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
